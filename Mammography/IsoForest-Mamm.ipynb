{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import argv\n",
    "from math import log, exp\n",
    "from random import randrange\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#--[Basic Function]---------------------------------------------------------------------\n",
    "#input decision_values, real_labels{1,-1}, #positive_instances, #negative_instances\n",
    "#output [A,B] that minimize sigmoid likilihood\n",
    "#refer to Platt's Probablistic Output for Support Vector Machines\n",
    "def SigmoidTrain(deci, label, A = None, B = None, prior0=None,prior1=None):\n",
    "    #Count prior0 and prior1 if needed\n",
    "    if prior1==None or prior0==None:\n",
    "        prior1, prior0 = 0, 0\n",
    "        for i in range(len(label)):\n",
    "            if label[i] > 0:\n",
    "                prior1+=1\n",
    "            else:\n",
    "                prior0+=1\n",
    "\n",
    "    #Parameter Setting\n",
    "    maxiter=1000\t#Maximum number of iterations\n",
    "    minstep=1e-10\t#Minimum step taken in line search\n",
    "    sigma=1e-12\t#For numerically strict PD of Hessian\n",
    "    eps=1e-5\n",
    "    length = len(deci)\n",
    "    \n",
    "    #Construct Target Support\n",
    "    hiTarget=(prior1+1.0)/(prior1+2.0)\n",
    "    #print(hiTarget)\n",
    "    loTarget=1/(prior0+2.0)\n",
    "    length=prior1+prior0\n",
    "    t=[]\n",
    "\n",
    "    for i in range(length):\n",
    "        if label[i] > 0:\n",
    "            t.append(hiTarget)\n",
    "        else:\n",
    "            t.append(loTarget)\n",
    "    #print(np.mean(t))\n",
    "    #Initial Point and Initial Fun Value\n",
    "    A,B=0.0, log((prior0+1.0)/(prior1+1.0))\n",
    "    #print(\"A,B\",A,B)\n",
    "    fval = 0.0\n",
    "\n",
    "    for i in range(length):\n",
    "        fApB = deci[i]*A+B\n",
    "        \n",
    "        if fApB >= 0: # Positive class hence label will be +1 \n",
    "            fval += t[i]*fApB + log(1+exp(-fApB))\n",
    "        else: # Negative class label will be -1\n",
    "            fval += (t[i] - 1)*fApB +log(1+exp(fApB))\n",
    "\n",
    "    for it in range(maxiter):\n",
    "        #Update Gradient and Hessian (use H' = H + sigma I)\n",
    "        h11=h22=sigma #Numerically ensures strict PD\n",
    "        h21=g1=g2=0.0\n",
    "        for i in range(length):\n",
    "            fApB = deci[i]*A+B\n",
    "            if (fApB >= 0):\n",
    "                p=exp(-fApB)/(1.0+exp(-fApB))\n",
    "                q=1.0/(1.0+exp(-fApB))\n",
    "            else:\n",
    "                p=1.0/(1.0+exp(fApB))\n",
    "                q=exp(fApB)/(1.0+exp(fApB))\n",
    "            d2=p*q\n",
    "            h11+=deci[i]*deci[i]*d2\n",
    "            h22+=d2\n",
    "            h21+=deci[i]*d2\n",
    "            d1=t[i]-p\n",
    "            g1+=deci[i]*d1\n",
    "            g2+=d1\n",
    "\n",
    "        #Stopping Criteria\n",
    "        if abs(g1)<eps and abs(g2)<eps:\n",
    "            break\n",
    "\n",
    "        #Finding Newton direction: -inv(H') * g\n",
    "        det=h11*h22-h21*h21\n",
    "        dA=-(h22*g1 - h21 * g2) / det\n",
    "        dB=-(-h21*g1+ h11 * g2) / det\n",
    "        gd=g1*dA+g2*dB\n",
    "\n",
    "        #Line Search\n",
    "        stepsize = 1\n",
    "        while stepsize >= minstep:\n",
    "            newA = A + stepsize * dA\n",
    "            newB = B + stepsize * dB\n",
    "\n",
    "            #New function value\n",
    "            newf = 0.0\n",
    "            for i in range(length):\n",
    "                fApB = deci[i]*newA+newB\n",
    "                if fApB >= 0:\n",
    "                    newf += t[i]*fApB + log(1+exp(-fApB))\n",
    "                else:\n",
    "                    newf += (t[i] - 1)*fApB +log(1+exp(fApB))\n",
    "\n",
    "            #Check sufficient decrease\n",
    "            if newf < fval + 0.0001 * stepsize * gd:\n",
    "                A, B, fval = newA, newB, newf\n",
    "                break\n",
    "            else:\n",
    "                stepsize = stepsize / 2.0\n",
    "\n",
    "        if stepsize < minstep:\n",
    "            print(\"line search fails\",A,B,g1,g2,dA,dB,gd)\n",
    "            \n",
    "            return [A,B]\n",
    "\n",
    "    if it>=maxiter-1:\n",
    "        print(\"reaching maximal iterations\",g1,g2)\n",
    "    return (A,B,fval)\n",
    "\n",
    "#reads decision_value and Platt parameter [A,B]\n",
    "#outputs predicted probability\n",
    "\n",
    "def SigmoidPredict(deci, AB):\n",
    "    A, B = AB\n",
    "    fApB = deci * A + B\n",
    "    if (fApB >= 0):\n",
    "        return exp(-fApB)/(1.0+exp(-fApB))\n",
    "    else:\n",
    "        return 1.0/(1+exp(fApB)) \n",
    "    return prob\n",
    "\n",
    "def Expectation(score, A, B):\n",
    "    t = []\n",
    "    for i in range(len(score)):\n",
    "        if A*score[i] + B >= 0:\n",
    "            t.append(1)\n",
    "        else:\n",
    "            t.append(0)\n",
    "    p = np.mean(t)\n",
    "    t[t == 0] = -1\n",
    "    #print(t)\n",
    "    return t\n",
    "\n",
    "\n",
    "def EM(score, A_init, B_init, prior0, prior1, maxit=1000, tol=1e-8):\n",
    "  # Estimation of parameter(Initial)\n",
    "    flag = 0\n",
    "    A_cur = A_init \n",
    "    B_cur = B_init\n",
    "    A_new = 0.0\n",
    "    B_new = 0.0\n",
    "  \n",
    "  # Iterate between expectation and maximization parts\n",
    "  \n",
    "    for i in range(maxit):\n",
    "        #print(i)\n",
    "        if(i != 0):\n",
    "            (A_new, B_new) = SigmoidTrain(score,Expectation(score, A_cur, B_cur), A_cur, B_cur)\n",
    "            #print(A_new, B_new)\n",
    "        else:\n",
    "            t = []\n",
    "            for i in range(len(score)):\n",
    "                if A_cur*score[i] + B_cur >= 0:\n",
    "                    t.append(1)\n",
    "                else:\n",
    "                    t.append(0)\n",
    "            t[t == 0] = -1\n",
    "            (A_new, B_new) = SigmoidTrain(score,t, A_cur, B_cur, prior0, prior1)\n",
    "            #print(A_new, B_new)    \n",
    "        \n",
    "        # Stop iteration if the difference between the current and new estimates is less than a tolerance level\n",
    "        if(A_cur - A_new < tol and B_cur - B_new < tol):\n",
    "            flag = 1\n",
    "            #break    \n",
    "        # Otherwise continue iteration\n",
    "        A_cur = A_new \n",
    "        B_cur = B_new\n",
    "    if(not flag):\n",
    "        print(\"Didn't converge\\n\")\n",
    "  \n",
    "    return (A_cur, B_cur)\n",
    "\n",
    "def SigmoidFitting(score, proportion):\n",
    "    fval = []\n",
    "    A = []\n",
    "    B = []\n",
    "    sorted_score = list(sorted(set(np.round(score, decimals = 2))))\n",
    "    for i in range(int(proportion*len(sorted_score))):            \n",
    "        threshold = sorted_score[i]\n",
    "        print(threshold)\n",
    "        t = [1 if j <= threshold else -1 for j in score]\n",
    "        (a, b, f) = SigmoidTrain(score,t)\n",
    "        A.append(a)\n",
    "        B.append(b)\n",
    "        fval.append(f)\n",
    "    return(A,B,fval)\n",
    "\n",
    "def SigmoidFittingGrid(score, proportion):\n",
    "    ngrid = score.shape[1]\n",
    "    fval = []\n",
    "    A = []\n",
    "    B = []\n",
    "    threshold = []\n",
    "    for param in range(ngrid):\n",
    "        a,b,f = SigmoidFitting(score[:,param], proportion)\n",
    "        fval.append(min(f))\n",
    "        A.append(a[np.argmin(f)])\n",
    "        B.append(b[np.argmin(f)])\n",
    "        threshold.append(score[np.argmin(f),param])\n",
    "    return A,B,fval,threshold\n",
    "        \n",
    "\n",
    "def ContextualForest(contexts, features, features_cat, features_num, gamma_range, ncontexts = None):\n",
    "    #contexts = shuffle(contexts)\n",
    "    if(not ncontexts):\n",
    "        ncontexts = contexts.shape[0]\n",
    "    context_scores = np.zeros((ncontexts,features.shape[0],len(gamma_range)))\n",
    "    for i in range(ncontexts):\n",
    "        context = list(contexts.iloc[i,])\n",
    "        print(context)\n",
    "        feature_names = list(features)\n",
    "        context_features = list()\n",
    "        behavioral_features = list()\n",
    "        \n",
    "        for feat in feature_names:\n",
    "                c = feat.split(\"_\")\n",
    "                if len(set(c).intersection(set(context))) > 0 or feat in context:\n",
    "                    context_features.append(feat)\n",
    "                else:\n",
    "                    behavioral_features.append(feat)\n",
    "        context_f = features[context_features]\n",
    "        behav_f = features[behavioral_features]\n",
    "        \n",
    "        # Finding the categorical and numerical features in context features.\n",
    "        if(features_cat != None):\n",
    "            cat_names = list(features_cat)\n",
    "            cat_context = list(set(context_f).intersection(set(cat_names)))\n",
    "            context_f_cat = context_f[cat_context]\n",
    "            num_names = list(features_num)\n",
    "            num_context = list(set(context_f).intersection(set(num_names)))\n",
    "            context_f_num = context_f[num_context]\n",
    "            # Finding the distances of the context space\n",
    "\n",
    "            cat_context_distance = metrics.pairwise.cosine_similarity(np.array(context_f_cat))\n",
    "            # Scaling the numerical data using MaxAbsScaler\n",
    "            context_f_num_scaled = MaxAbsScaler().fit_transform(np.array(context_f_num))\n",
    "            # Zero mean and unit variance scaling\n",
    "            #context_f_num_scaled = preprocessing.scale(context_f_num)\n",
    "            #num_context_distance = metrics.pairwise.euclidean_distances(context_f_num_scaled)\n",
    "\n",
    "            #print(\"Cat distance\",cat_context_distance)\n",
    "            for gamma in range(len(gamma_range)): \n",
    "                print(gamma_range[gamma])\n",
    "                num_context_distance = metrics.pairwise.rbf_kernel(context_f_num_scaled, gamma= gamma_range[gamma])\n",
    "                #print(\"Num distance\",num_context_distance)\n",
    "                context_distance = np.minimum(cat_context_distance,num_context_distance)\n",
    "                #context_distance = num_context_distance\n",
    "                rng = np.random.RandomState(42)\n",
    "                clf = IsolationForest(max_samples=256, random_state=rng, smoothing = True)\n",
    "                clf.fit(behav_f, context_distance)\n",
    "                context_scores[i,:,gamma] = clf.decision_function(behav_f,distance=context_distance)\n",
    "        else:\n",
    "            num_names = list(features_num)\n",
    "            num_context = list(set(context_f).intersection(set(num_names)))\n",
    "            context_f_num = context_f[num_context]\n",
    "            # Finding the distances of the context space\n",
    "\n",
    "            # Scaling the numerical data using MaxAbsScaler\n",
    "            context_f_num_scaled = MaxAbsScaler().fit_transform(np.array(context_f_num))\n",
    "            # Zero mean and unit variance scaling\n",
    "            #context_f_num_scaled = preprocessing.scale(context_f_num)\n",
    "            #num_context_distance = metrics.pairwise.euclidean_distances(context_f_num_scaled)\n",
    "\n",
    "            #print(\"Cat distance\",cat_context_distance)\n",
    "            for gamma in range(len(gamma_range)): \n",
    "                print(gamma_range[gamma])\n",
    "                num_context_distance = metrics.pairwise.rbf_kernel(context_f_num_scaled, gamma= gamma_range[gamma])\n",
    "                #print(\"Num distance\",num_context_distance)\n",
    "                #context_distance = np.minimum(cat_context_distance,num_context_distance)\n",
    "                context_distance = num_context_distance\n",
    "                rng = np.random.RandomState(42)\n",
    "                clf = IsolationForest(max_samples=256, random_state=rng, smoothing = True)\n",
    "                clf.fit(behav_f, context_distance)\n",
    "                context_scores[i,:,gamma] = clf.decision_function(behav_f,distance=context_distance)           \n",
    "        \n",
    "    return context_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Play Arena Test times of the implementation.\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# rng = np.random.RandomState(42)\n",
    "\n",
    "# # Generate train data\n",
    "# X = 0.3 * rng.randn(5000, 2)\n",
    "# X_train = np.r_[X + 2, X - 2]\n",
    "\n",
    "# # fit the iForest model\n",
    "# distance = np.ones((X_train.shape[0], X_train.shape[0]))\n",
    "# clf = IsolationForest(random_state=rng, smoothing = False)\n",
    "# import timeit\n",
    "# start_time = timeit.default_timer()\n",
    "# clf.fit(X_train, distance)\n",
    "# y_pred_train = clf.decision_function(X_train, distance)\n",
    "# print(\"iForest:\",timeit.default_timer() - start_time)\n",
    "# # fit the cForest model\n",
    "# distance = np.ones((X_train.shape[0], X_train.shape[0]))\n",
    "\n",
    "# clf = IsolationForest(random_state=rng, smoothing = True)\n",
    "# import timeit\n",
    "# start_time = timeit.default_timer()\n",
    "# clf.fit(X_train, distance)\n",
    "# y_pred_train = clf.decision_function(X_train, distance)\n",
    "# print(\"cForest:\",timeit.default_timer() - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the feature matrix and the context from the UnifiedMeasure.\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pandas2ri.activate()\n",
    "\n",
    "# Feat matrix 2 will need to be saved as data frame in the R script.\n",
    "readRDS = robjects.r['readRDS']\n",
    "df_feat = readRDS('features_mamm.RDS')\n",
    "df_feat = pandas2ri.ri2py_dataframe(df_feat)\n",
    "#df_feat = pd.DataFrame(np.transpose(df_feat))\n",
    "\n",
    "# Contexts are being stored as data frames in R.\n",
    "df_contexts = readRDS('contextmamms.RDS')\n",
    "df_contexts = pandas2ri.ri2py_dataframe(df_contexts) \n",
    "df_contexts = pd.DataFrame(df_contexts)\n",
    "\n",
    "# Loading typevar \n",
    "df_typevar = readRDS('typevar_mamm.RDS')\n",
    "df_typevar = pandas2ri.ri2py_dataframe(df_typevar) \n",
    "\n",
    "# Loading ground truth \n",
    "labels = readRDS('labels_mamm.RDS')\n",
    "labels = pandas2ri.ri2py(labels)\n",
    "df_contexts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting dummies for isolation forest input\n",
    "#test = pd.get_dummies(df_feat)\n",
    "df_typevar.head(10)\n",
    "categorical = df_typevar[df_typevar['typevar'] == \"categorical\"].index.tolist()\n",
    "other = df_typevar[df_typevar['typevar'] != \"categorical\"].index.tolist()\n",
    "# adjusting for indices in py\n",
    "categorical = [i-1 for i in categorical]\n",
    "other = [i-1 for i in other]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling for no categorical features. \n",
    "if(len(categorical)):\n",
    "    df_feat_cat = df_feat.iloc[:,categorical]\n",
    "    df_feat_other = df_feat.iloc[:,other]\n",
    "    df_feat_cat = pd.get_dummies(df_feat_cat)\n",
    "    df_feat_all = pd.concat([df_feat_other, df_feat_all], axis=1)\n",
    "else:\n",
    "    df_feat_other = df_feat.iloc[:,other]\n",
    "    df_feat_all = df_feat_other\n",
    "    df_feat_cat = None\n",
    "# The full feature list in built and ready to be passed to iForest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_range = [0.0001,0.001,0.01,0.1,1,10,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "scores = ContextualForest(df_contexts, df_feat_all, df_feat_cat, df_feat_other,\n",
    "                          gamma_range = gamma_range)\n",
    "print(timeit.default_timer() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savez(\"scores_r5.npz\",scores)\n",
    "np.savez(\"scores_all_mamms.npz\",scores)\n",
    "#scores = np.load(\"scores.npz\")[\"arr_0\"]\n",
    "#fscores = np.median(scores, axis = 0)\n",
    "scores #Each row corresponds to a context with varying gammas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating the scores\n",
    "fscores = np.median(scores, axis = 0)\n",
    "fscores = (fscores - np.amin(fscores, axis = 0))/(np.amax(fscores,axis =0) - np.amin(fscores,axis = 0))\n",
    "fscores = np.array(fscores)\n",
    "fscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ABft = SigmoidFittingGrid(fscores, proportion = 0.1)\n",
    "print(ABft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 4.7330971860007836 B: 5.920800826766766 f: [15.101539974758671, 15.101556106691442, 15.101726523143089, 15.11949376066417, 15.165323335805919]\n",
      "Threshold: 0.2287651003834251\n",
      "Gamma Chosen: 0.001\n"
     ]
    }
   ],
   "source": [
    "A,B,f,t = ABft\n",
    "A,B,fmin = A[np.argmin(f)],B[np.argmin(f)],min(f)\n",
    "print(\"A:\",A,\"B:\",B,\"f:\",f)\n",
    "print(\"Threshold:\",t[np.argmin(f)])\n",
    "print(\"Gamma Chosen:\",gamma_range[np.argmin(f)])\n",
    "fgscores = fscores[:,np.argmin(f)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABf = SigmoidFitting(fscores, proportion = 0.2)\n",
    "print(ABf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "A,B,f = ABf\n",
    "A,B,f = A[np.argmin(f)],B[np.argmin(f)],min(f)\n",
    "print(\"A:\",A,\"B:\",B,\"f:\",f)\n",
    "print(\"Threshold:\",fscores[np.argmin(f)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "\n",
    "inliers = fgscores[np.where(labels == 0)]\n",
    "outliers = fgscores[np.where(labels == 1)]\n",
    "\n",
    "pyplot.hist(inliers,  alpha=0.5, label='inliers')\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show()\n",
    "\n",
    "pyplot.hist(outliers,  alpha=0.5, label='outliers')\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show()\n",
    "\n",
    "import seaborn as sns\n",
    "fig, ax = plt.subplots()\n",
    "sns.distplot(outliers, hist=False, label = \"outliers\")\n",
    "sns.distplot(inliers, hist=False, label = \"inliers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fgscores[np.where(labels == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00024255858691346862"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = []\n",
    "for i in range(len(fscores)):\n",
    "    prob.append(SigmoidPredict(fgscores[i],(A,B)))\n",
    "np.mean(prob)\n",
    "#prob = (prob - np.min(prob))/(np.max(prob) - np.min(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Fixing random state for reproducibility\n",
    "np.random.seed(19680801)\n",
    "\n",
    "# the histogram of the data\n",
    "#n, bins, patches = plt.hist(prob, 50, normed=1, facecolor='g', alpha=0.75)\n",
    "plt.scatter(fgscores, prob, c=\"g\", alpha=0.5)\n",
    "plt.xlim((0,1))\n",
    "plt.ylim((0,1))\n",
    "plt.ylabel('Probability Estimate')\n",
    "plt.xlabel('Contextual iForest Score')\n",
    "plt.title(\"Fraud Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Context iForest\n",
    "rng = np.random.RandomState(42)\n",
    "clf = IsolationForest(max_samples=256, random_state=rng, smoothing = False)\n",
    "distance = np.ones((len(df_feat_all), len(df_feat_all)), order = \"f\")\n",
    "clf.fit(df_feat_all, distance)\n",
    "ifscores = clf.decision_function(df_feat_all,distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoF analysis\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "clf = LocalOutlierFactor(n_neighbors=200)\n",
    "y_pred = clf.fit_predict(df_feat_all)\n",
    "lof_scores = clf.negative_outlier_factor_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "f, axes = plt.subplots(1, 1, figsize=(7, 5))\n",
    "y_real = list(labels)\n",
    "y_proba = []\n",
    "fscores = np.median(scores, axis = 0)\n",
    "fscores = (fscores - np.amin(fscores, axis = 0))/(np.amax(fscores,axis =0) - np.amin(fscores,axis = 0))\n",
    "fscores = np.array(fscores)\n",
    "for i in range(len(gamma_range)):\n",
    "    precision, recall, _ = precision_recall_curve(labels, -1*fscores[:,i])\n",
    "    lab = 'Gamma: %f AUC=%.4f' % (gamma_range[i], auc(recall, precision))\n",
    "    axes.step(recall, precision, label=lab)\n",
    "    y_real.append(labels)\n",
    "    y_proba.append(-1*fscores[:,i])\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(labels, -1*ifscores)\n",
    "y_real.append(labels)\n",
    "y_proba.append(-1*ifscores)\n",
    "lab = 'No Context iForest AUC=%.4f' % (auc(recall, precision))\n",
    "axes.step(recall, precision, label=lab, lw=2, color='black')\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(labels, -1*lof_scores)\n",
    "y_real.append(labels)\n",
    "y_proba.append(-1*lof_scores)\n",
    "lab = 'No Context LOF AUC=%.4f' % (auc(recall, precision))\n",
    "axes.step(recall, precision, label=lab, lw=2)\n",
    "\n",
    "axes.set_xlabel('Recall')\n",
    "axes.set_ylabel('Precision')\n",
    "axes.legend(loc='upper right', fontsize='small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "f, axes = plt.subplots(1, 1, figsize=(7, 5))\n",
    "y_real = list(labels)\n",
    "y_proba = []\n",
    "# fscores = np.min(scores, axis = 0)\n",
    "# fscores = (fscores - np.amin(fscores, axis = 0))/(np.amax(fscores,axis =0) - np.amin(fscores,axis = 0))\n",
    "# fscores = np.array(fscores)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(labels, -1*fgscores)\n",
    "lab = 'ContextualForest: AUC=%.4f' % ( auc(recall, precision))\n",
    "axes.step(recall, precision, label=lab)\n",
    "y_real.append(labels)\n",
    "y_proba.append(-1*fgscores)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(labels, -1*ifscores)\n",
    "y_real.append(labels)\n",
    "y_proba.append(-1*ifscores)\n",
    "lab = 'iForest: AUC=%.4f' % (auc(recall, precision))\n",
    "axes.step(recall, precision, label=lab, lw=2, color='black')\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(labels, -1*lof_scores)\n",
    "y_real.append(labels)\n",
    "y_proba.append(-1*lof_scores)\n",
    "lab = 'LOF: AUC=%.4f' % (auc(recall, precision))\n",
    "axes.step(recall, precision, label=lab, lw=2, color='red')\n",
    "\n",
    "axes.set_xlabel('Recall')\n",
    "axes.set_ylabel('Precision')\n",
    "axes.legend(loc='upper right', fontsize='small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcontscores = np.load(\"scores_nocat.npz\")['arr_0']\n",
    "frscores = np.min(rcontscores, axis = 0)\n",
    "#frscores = (frscores - np.amin(frscores, axis = 0))/(np.amax(frscores,axis =0) - np.amin(frscores,axis = 0))\n",
    "frscores = np.array(frscores)\n",
    "fgrscores = frscores[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "h, axes = plt.subplots(1, 1, figsize=(7, 5))\n",
    "y_real = list(labels)\n",
    "y_proba = []\n",
    "# scores = np.load(\"scores_adobe_all_red.npz\")[\"arr_0\"]\n",
    "# fscores = (scores - np.amin(scores, axis = 1, keepdims= True))/(np.amax(scores,axis =1, keepdims=True) - np.amin(scores,axis = 1,\n",
    "#                                                                                                                 keepdims=True))\n",
    "# fscores = np.min(fscores, axis = 0)\n",
    "# fscores = np.array(fscores)\n",
    "# fgscores = fscores[:,np.argmin(f)]\n",
    "\n",
    "# precision, recall, _ = precision_recall_curve(labels, -1*fgscores)\n",
    "# lab = 'ContextualForest (Reduced): AUC=%.4f' % ( auc(recall, precision))\n",
    "# axes.step(recall, precision, label=lab, color='green')\n",
    "# y_real.append(labels)\n",
    "# y_proba.append(-1*fgscores)\n",
    "\n",
    "# scores = np.load(\"scores_adobe_all.npz\")[\"arr_0\"]\n",
    "# fscores = (scores - np.amin(scores, axis = 1, keepdims= True))/(np.amax(scores,axis =1, keepdims=True) - np.amin(scores,axis = 1,\n",
    "#                                                                                                                 keepdims=True))\n",
    "# fscores = np.min(fscores, axis = 0)\n",
    "# fscores = np.array(fscores)\n",
    "# fgscores = fscores[:,np.argmin(f)]\n",
    "\n",
    "# precision, recall, _ = precision_recall_curve(labels, -1*fgscores)\n",
    "# lab = 'ContextualForest : AUC=%.4f' % ( auc(recall, precision))\n",
    "# axes.step(recall, precision, label=lab, color='red')\n",
    "# y_real.append(labels)\n",
    "# y_proba.append(-1*fgscores)\n",
    "\n",
    "# rng = np.random.RandomState(42)\n",
    "# clf = IsolationForest(max_samples=256, random_state=rng, n_estimators=5400, smoothing = False)\n",
    "# distance = np.ones((len(df_feat_all), len(df_feat_all)), order = \"f\")\n",
    "# clf.fit(df_feat_all, distance)\n",
    "# ifscores = clf.decision_function(df_feat_all,distance)\n",
    "\n",
    "# precision, recall, _ = precision_recall_curve(labels, -1*ifscores)\n",
    "# lab = 'iForest with 100*K estimators: AUC=%.4f' % ( auc(recall, precision))\n",
    "# axes.step(recall, precision, label=lab,  lw = 2)\n",
    "# y_real.append(labels)\n",
    "# y_proba.append(-1*ifscores)\n",
    "\n",
    "# rng = np.random.RandomState(42)\n",
    "# clf = IsolationForest(max_samples=256, random_state=rng, n_estimators=100, smoothing = False)\n",
    "# distance = np.ones((len(df_feat_all), len(df_feat_all)), order = \"f\")\n",
    "# clf.fit(df_feat_all, distance)\n",
    "# ifscores = clf.decision_function(df_feat_all,distance)\n",
    "\n",
    "# precision, recall, _ = precision_recall_curve(labels, -1*ifscores)\n",
    "# y_real.append(labels)\n",
    "# y_proba.append(-1*ifscores)\n",
    "# lab = 'iForest 100 estimators: AUC=%.4f' % (auc(recall, precision))\n",
    "# axes.step(recall, precision, label=lab, lw=2, color='black')\n",
    "\n",
    "# # LoF analysis\n",
    "\n",
    "# from sklearn.neighbors import LocalOutlierFactor\n",
    "# clf = LocalOutlierFactor(n_neighbors=25)\n",
    "# y_pred = clf.fit_predict(df_feat_all)\n",
    "# lof_scores = clf.negative_outlier_factor_\n",
    "\n",
    "# precision, recall, _ = precision_recall_curve(labels, -1*lof_scores)\n",
    "# y_real.append(labels)\n",
    "# y_proba.append(-1*lof_scores)\n",
    "# lab = 'LOF (25 neighbors): AUC=%.4f' % (auc(recall, precision))\n",
    "# axes.step(recall, precision, label=lab, lw=2)\n",
    "\n",
    "# from sklearn.neighbors import LocalOutlierFactor\n",
    "# clf = LocalOutlierFactor(n_neighbors=50)\n",
    "# y_pred = clf.fit_predict(df_feat_all)\n",
    "# lof_scores = clf.negative_outlier_factor_\n",
    "\n",
    "# precision, recall, _ = precision_recall_curve(labels, -1*lof_scores)\n",
    "# y_real.append(labels)\n",
    "# y_proba.append(-1*lof_scores)\n",
    "# lab = 'LOF (50 neighbors): AUC=%.4f' % (auc(recall, precision))\n",
    "# axes.step(recall, precision, label=lab, lw=2)\n",
    "\n",
    "# # LoF analysis\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "clf = LocalOutlierFactor(n_neighbors=100)\n",
    "y_pred = clf.fit_predict(df_feat_all)\n",
    "lof_scores = clf.negative_outlier_factor_\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(labels, -1*lof_scores)\n",
    "y_real.append(labels)\n",
    "y_proba.append(-1*lof_scores)\n",
    "lab = 'LOF (100 neighbors): AUC=%.4f' % (auc(recall, precision))\n",
    "axes.step(recall, precision, label=lab, lw=2)\n",
    "\n",
    "# LoF analysis\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "clf = LocalOutlierFactor(n_neighbors=150)\n",
    "y_pred = clf.fit_predict(df_feat_all)\n",
    "lof_scores = clf.negative_outlier_factor_\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(labels, -1*lof_scores)\n",
    "y_real.append(labels)\n",
    "y_proba.append(-1*lof_scores)\n",
    "lab = 'LOF (150 neighbors): AUC=%.4f' % (auc(recall, precision))\n",
    "axes.step(recall, precision, label=lab, lw=2)\n",
    "\n",
    "#Elliptic Covariance\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "clf = EllipticEnvelope(contamination=0.1)\n",
    "clf.fit(df_feat_all)\n",
    "ell_scores = clf.decision_function(df_feat_all)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(labels, -1*ell_scores)\n",
    "y_real.append(labels)\n",
    "y_proba.append(-1*ell_scores)\n",
    "lab = 'Ellpitic Covaraince: AUC=%.4f' % (auc(recall, precision))\n",
    "axes.step(recall, precision, label=lab, lw=2)\n",
    "\n",
    "\n",
    "# One class SVM\n",
    "from sklearn import svm\n",
    "clf = svm.OneClassSVM(kernel=\"rbf\")\n",
    "clf.fit(df_feat_all)\n",
    "svm_scores = clf.decision_function(df_feat_all)\n",
    "precision, recall, _ = precision_recall_curve(labels, -1*svm_scores)\n",
    "y_real.append(labels)\n",
    "y_proba.append(-1*svm_scores)\n",
    "lab = 'oSVM: AUC=%.4f' % (auc(recall, precision))\n",
    "axes.step(recall, precision, label=lab, lw=2)\n",
    "\n",
    "axes.set_xlabel('Recall')\n",
    "axes.set_ylabel('Precision')\n",
    "axes.legend(loc='upper right', fontsize='small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
